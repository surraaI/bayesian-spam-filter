{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('sms+spam+collection/SMSSpamCollection', sep='\\t', header=None, names=['label', 'message'])\n",
    "\n",
    "\n",
    "print(data.head())\n",
    "print(data['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and tokenize the messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...   \n",
      "1   ham                      Ok lar... Joking wif u oni...   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3   ham  U dun say so early hor... U c already then say...   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                   processed_message  \n",
      "0  [go, until, jurong, point, crazy, available, o...  \n",
      "1                     [ok, lar, joking, wif, u, oni]  \n",
      "2  [free, entry, in, 2, a, wkly, comp, to, win, f...  \n",
      "3  [u, dun, say, so, early, hor, u, c, already, t...  \n",
      "4  [nah, i, dont, think, he, goes, to, usf, he, l...  \n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  \n",
    "    words = text.split() \n",
    "    return words\n",
    "\n",
    "data['processed_message'] = data['message'].apply(preprocess_text)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "\n",
    "# Separate features and labels for training and testing\n",
    "train_messages = train_data['message']\n",
    "train_labels = train_data['label']\n",
    "test_messages = test_data['message']\n",
    "test_labels = test_data['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(spam): 0.1341025641025641, P(ham): 0.865897435897436\n"
     ]
    }
   ],
   "source": [
    "# Calculate priors\n",
    "total_messages = len(train_data)\n",
    "spam_messages = len(train_data[train_data['label'] == 'spam'])\n",
    "ham_messages = len(train_data[train_data['label'] == 'ham'])\n",
    "\n",
    "prior_spam = spam_messages / total_messages\n",
    "prior_ham = ham_messages / total_messages\n",
    "\n",
    "print(f\"P(spam): {prior_spam}, P(ham): {prior_ham}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word frequencies for spam and ham\n",
    "spam_words = train_data[train_data['label'] == 'spam']['processed_message'].explode()\n",
    "ham_words = train_data[train_data['label'] == 'ham']['processed_message'].explode()\n",
    "\n",
    "spam_word_counts = spam_words.value_counts()\n",
    "ham_word_counts = ham_words.value_counts()\n",
    "\n",
    "# Vocabulary size and total words\n",
    "vocab_size = len(set(spam_words).union(set(ham_words)))\n",
    "total_spam_words = spam_words.count()\n",
    "total_ham_words = ham_words.count()\n",
    "\n",
    "# Likelihood calculation with Laplace smoothing\n",
    "def likelihood(word, word_counts, total_words, alpha=1):\n",
    "    return (word_counts.get(word, 0) + alpha) / (total_words + alpha * vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute posterior probabilities and classify\n",
    "def classify_message(message):\n",
    "    words = preprocess_text(message)\n",
    "    spam_prob = np.log(prior_spam)\n",
    "    ham_prob = np.log(prior_ham)\n",
    "    \n",
    "    for word in words:\n",
    "        spam_prob += np.log(likelihood(word, spam_word_counts, total_spam_words))\n",
    "        ham_prob += np.log(likelihood(word, ham_word_counts, total_ham_words))\n",
    "    \n",
    "    return 'spam' if spam_prob > ham_prob else 'ham'\n",
    "\n",
    "# Apply classification\n",
    "train_data['predicted_label'] = train_data['message'].apply(classify_message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9778708133971292\n",
      "Precision: 0.8945147679324894\n",
      "Recall: 0.9464285714285714\n",
      "F1 Score: 0.9197396963123644\n",
      "Confusion Matrix:\n",
      "[[1423   25]\n",
      " [  12  212]]\n"
     ]
    }
   ],
   "source": [
    "# Classify messages in the test set\n",
    "test_data['predicted_label'] = test_data['message'].apply(classify_message)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(test_labels, test_data['predicted_label'])\n",
    "precision = precision_score(test_labels, test_data['predicted_label'], pos_label='spam')\n",
    "recall = recall_score(test_labels, test_data['predicted_label'], pos_label='spam')\n",
    "f1 = f1_score(test_labels, test_data['predicted_label'], pos_label='spam')\n",
    "conf_matrix = confusion_matrix(test_labels, test_data['predicted_label'])\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
